{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# initialize the set of links (unique links)\n",
    "internal_urls = set()\n",
    "\n",
    "total_urls_visited = 0\n",
    "\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_all_website_links(url):\n",
    "    \"\"\"\n",
    "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
    "    \"\"\"\n",
    "    # all URLs of `url`\n",
    "    urls = set()\n",
    "    # domain name of the URL without the protocol\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "        # join the URL if it's relative (not absolute link)\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        # remove URL GET parameters, URL fragments, etc.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            # not a valid URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # already in the set\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # external link\n",
    "            continue\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def crawl(url, max_urls):\n",
    "    \"\"\"\n",
    "    Crawls a web page and extracts all links.\n",
    "    You'll find all links in `external_urls` and `internal_urls` global set variables.\n",
    "    params:\n",
    "        max_urls (int): number of max urls to crawl, default is 50.\n",
    "    \"\"\"\n",
    "    global total_urls_visited\n",
    "    total_urls_visited += 1\n",
    "    links = get_all_website_links(url)\n",
    "    for link in links:\n",
    "        if total_urls_visited > max_urls:\n",
    "            break\n",
    "        crawl(link, max_urls=max_urls)\n",
    "        \n",
    "def link_extractor(url, max_urls=50):\n",
    "    crawl(url, max_urls)\n",
    "    print(\"[+] Internal links found:\", len(internal_urls))\n",
    "    domain_name = urlparse(url).netloc\n",
    "\n",
    "    # save the internal links to a file\n",
    "    with open(f\"{domain_name}_internal_links.txt\", \"w\") as f:\n",
    "        for internal_link in internal_urls:\n",
    "            print(internal_link.strip(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Internal links found: 2789\n"
     ]
    }
   ],
   "source": [
    "link_extractor('https://apkpure.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# initialize the set of links (unique links)\n",
    "downloaded_apk = set()\n",
    "\n",
    "def download(link):\n",
    "    res = requests.get(link + '/download?from=details', headers={\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.5 (KHTML, like Gecko) '\n",
    "                      'Version/9.1.2 Safari/601.7.5 '\n",
    "    }).text\n",
    "    soup = BeautifulSoup(res, \"html.parser\").find('a', {'id': 'download_link'})\n",
    "    if soup['href']:\n",
    "        r = requests.get(soup['href'], stream=True, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.5 (KHTML, like Gecko) '\n",
    "                          'Version/9.1.2 Safari/601.7.5 '\n",
    "        })\n",
    "        with open(link.split('/')[-1] + '.apk', 'wb') as file:\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        \n",
    "def download_apk(filename):\n",
    "    # Open the file with read only permit\n",
    "    f = open(filename)\n",
    "    # use readline() to read the first line \n",
    "    download_link = f.readline()\n",
    "    counter = 0\n",
    "    # use the read line to read further.\n",
    "    # If the file is not empty keep reading one line\n",
    "    # at a time, till the file is empty\n",
    "    while download_link:\n",
    "        download_link = download_link.replace('\\n','')\n",
    "        \n",
    "        if download_link is not None:\n",
    "            print('Downloading {}.apk ...'.format(download_link))\n",
    "            try:\n",
    "            # block raising an exception\n",
    "                download(download_link)\n",
    "                # save the filename to a file\n",
    "                with open(f\"filenames.txt\", \"a\") as g:\n",
    "                    g.write('\\n')\n",
    "                    g.write(download_link.split('/')[-1] + '.apk')\n",
    "                counter+=1\n",
    "                print('Download completed!')\n",
    "                g.close()\n",
    "            except:\n",
    "                pass # doing nothing on exception    \n",
    "        else:\n",
    "            print('No results')\n",
    "        # use realine() to read next line\n",
    "        download_link = f.readline()\n",
    "    print(counter)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://apkpure.com/lulubox/com.lulu.lulubox.apk ...\n",
      "Download completed!\n",
      "Downloading https://apkpure.com/br/facebook-lite/com.facebook.lite.apk ...\n",
      "Download completed!\n",
      "Downloading https://apkpure.com/de/messenger-lite-free-calls-messages/com.facebook.mlite.apk ...\n",
      "Download completed!\n",
      "Downloading https://apkpure.com/de/twitter-lite/com.twitter.android.lite.apk ...\n",
      "Download completed!\n",
      "Downloading https://apkpure.com/de/line-lite-free-calls-messages/com.linecorp.linelite.apk ...\n",
      "Download completed!\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "download_apk('untitled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
