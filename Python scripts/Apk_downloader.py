#!/usr/bin/env python
# coding: utf-8

# In[3]:


import requests
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup

# initialize the set of links (unique links)
internal_urls = set()

total_urls_visited = 0


def is_valid(url):
    """
    Checks whether `url` is a valid URL.
    """
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)


def get_all_website_links(url):
    """
    Returns all URLs that is found on `url` in which it belongs to the same website
    """
    # all URLs of `url`
    urls = set()
    # domain name of the URL without the protocol
    domain_name = urlparse(url).netloc
    soup = BeautifulSoup(requests.get(url).content, "html.parser")
    for a_tag in soup.findAll("a"):
        href = a_tag.attrs.get("href")
        if href == "" or href is None:
            # href empty tag
            continue
        # join the URL if it's relative (not absolute link)
        href = urljoin(url, href)
        parsed_href = urlparse(href)
        # remove URL GET parameters, URL fragments, etc.
        href = parsed_href.scheme + "://" + parsed_href.netloc + parsed_href.path
        if not is_valid(href):
            # not a valid URL
            continue
        if href in internal_urls:
            # already in the set
            continue
        if domain_name not in href:
            # external link
            continue
        urls.add(href)
        internal_urls.add(href)
    return urls


def crawl(url, max_urls):
    """
    Crawls a web page and extracts all links.
    You'll find all links in `external_urls` and `internal_urls` global set variables.
    params:
        max_urls (int): number of max urls to crawl, default is 50.
    """
    global total_urls_visited
    total_urls_visited += 1
    links = get_all_website_links(url)
    for link in links:
        if total_urls_visited > max_urls:
            break
        crawl(link, max_urls=max_urls)
        
def link_extractor(url, max_urls=500):
    crawl(url, max_urls)
    print("[+] Internal links found:", len(internal_urls))
    domain_name = urlparse(url).netloc

    # save the internal links to a file
    with open(f"{domain_name}_internal_links.txt", "w") as f:
        for internal_link in internal_urls:
            print(internal_link.strip(), file=f)


# In[4]:


link_extractor('https://apkpure.com/')


# In[9]:


from bs4 import BeautifulSoup
import requests

# initialize the set of links (unique links)
downloaded_apk = set()

def download(link):
    res = requests.get(link + '/download?from=details', headers={
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.5 (KHTML, like Gecko) '
                      'Version/9.1.2 Safari/601.7.5 '
    }).text
    soup = BeautifulSoup(res, "html.parser").find('a', {'id': 'download_link'})
    if soup['href']:
        r = requests.get(soup['href'], stream=True, headers={
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.5 (KHTML, like Gecko) '
                          'Version/9.1.2 Safari/601.7.5 '
        })
        with open(link.split('/')[-1] + '.apk', 'wb') as file:
            for chunk in r.iter_content(chunk_size=1024):
                if chunk:
                    file.write(chunk)
        
def download_apk(filename):
    # Open the file with read only permit
    f = open(filename)
    # use readline() to read the first line 
    download_link = f.readline()
    counter = 0
    # use the read line to read further.
    # If the file is not empty keep reading one line
    # at a time, till the file is empty
    while download_link:
        download_link = download_link.replace('\n','')
        
        if download_link is not None:
            print('Downloading {}.apk ...'.format(download_link))
            try:
            # block raising an exception
                download(download_link)
                # save the filename to a file
                with open(f"filenames.txt", "a") as g:
                    g.write('\n')
                    g.write(download_link.split('/')[-1] + '.apk')
                counter+=1
                print('Download completed!')
                g.close()
            except:
                pass # doing nothing on exception    
        else:
            print('No results')
        # use realine() to read next line
        download_link = f.readline()
    print(counter)
    f.close()


# In[10]:


download_apk('apkpure.com_internal_links.txt')


# In[ ]:




